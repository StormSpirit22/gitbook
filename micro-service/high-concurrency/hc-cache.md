# 缓存篇

## 基础

### 缓存案例

Linux 内存管理是通过一个叫做 MMU（Memory Management Unit）的硬件，来实现从虚拟地址到物理地址的转换的，但是如果每次转换都要做这么复杂计算的话，无疑会造成性能的损耗，所以我们会借助一个叫做 TLB（Translation Lookaside Buffer）的组件来缓存最近转换过的虚拟地址，和物理地址的映射。TLB 就是一种缓存组件，缓存复杂运算的结果，就好比你做一碗色香味俱全的面条可能比较复杂，那么我们把做好的面条油炸处理一下做成方便面，你做方便面的话就简单多了，也快速多了。这个缓存组件比较底层，这里你只需要了解一下就可以了。

在大部分的笔记本，桌面电脑和服务器上都会有一个或者多个 TLB 组件，在不经意间帮助我们加快地址转换的速度。

**除此之外，我们熟知的 HTTP 协议也是有缓存机制的。**当我们第一次请求静态的资源时，比如一张图片，服务端除了返回图片信息，在响应头里面还有一个 `Etag` 的字段。浏览器会缓存图片信息以及这个字段的值。当下一次再请求这个图片的时候，浏览器发起的请求头里面会有一个 `If-None-Match` 的字段，并且把缓存的 `Etag` 的值写进去发给服务端。服务端比对图片信息是否有变化，如果没有，则返回浏览器一个 304 的状态码，浏览器会继续使用缓存的图片信息。通过这种缓存协商的方式，可以减少网络传输的数据大小，从而提升页面展示的性能。

![img](https://zq99299.github.io/note-architect/assets/img/7a2344bd27535936b4ad4d8519d9fd81.7a2344bd.jpg)

#### http 缓存

参考 

[5分钟看懂系列：HTTP缓存机制详解](https://segmentfault.com/a/1190000021716418)

[HTTP 缓存](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Caching)

![img](https://image-static.segmentfault.com/155/525/1555253303-5b4b22be9160d_articlex)

- HTTP缓存主要分强制缓存和对比缓存
- 强制缓存的HTTP相关头部Cache-Control（HTTP 1.1），Exipres（HTTP1.0）,浏览器直接读本地缓存，不会再跟服务器端交互，状态码200。
- 对比缓存的HTTP相关头部Last-Modified / If-Modified-Since， Etag / If-None-Match (优先级比Last-Modified / If-Modified-Since高)，每次请求需要让服务器判断一下资源是否更新过，从而决定浏览器是否使用缓存，如果是，则返回304，否则重新完整响应。

### 缓存分类

在我们日常开发中，常见的缓存主要就是 **静态缓存、分布式缓存和热点本地缓存** 这三种。

对于静态的资源的缓存你可以选择静态缓存：缓存在 nginx 上。

对于动态的请求你可以选择分布式缓存：Redis， Memcached。

对于极端的热点数据查询的时候可以选择本地缓存： bigCache 等。



## 缓存的读写策略

### Cache Aside（旁路缓存）策略

这个策略数据 **以数据库中的数据为准，缓存中的数据是按需加载的** 。它可以分为读策略和写策略，

- **读的时候**，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- **更新的时候**，先更新数据库，然后再删除缓存。

![image-20220309155440011](/Users/tianyou/Library/Application Support/typora-user-images/image-20220309155440011.png)

以下内容来自 [ 数据库和缓存一致性](https://pdai.tech/md/db/nosql-redis/db-redis-x-cache.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%90%86%E8%A7%A3redis%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98)

Cache Aside 也会有并发问题，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

可以使用以下方法来更新缓存：

#### 方案：异步更新缓存(基于订阅binlog的同步机制)

![image-20220309153447320](/Users/tianyou/Library/Application Support/typora-user-images/image-20220309153447320.png)

1. **技术整体思路**：

MySQL binlog增量订阅消费+消息队列+增量数据更新到redis

1）读Redis：热数据基本都在Redis

2）写MySQL: 增删改都是操作MySQL

3）更新Redis数据：MySQ的数据操作binlog，来更新到Redis

1. **Redis更新**

1）**数据操作**主要分为两大块：

- 一个是全量(将全部数据一次写入到redis)
- 一个是增量（实时更新）

这里说的是增量,指的是mysql的update、insert、delate变更数据。

2）**读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据**。

这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。

其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。

这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。

当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis

### Read/Write Through（读穿 / 写穿）策略

**这个策略的核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据** 。

先查询要写入的数据在缓存中是否已经存在，如果已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，如果缓存中数据不存在（ Write Miss（写失效）），可以选择先写入缓存再由缓存更新到数据库方式（Write Allocate（按写分配））或者直接更新到数据库，不更新缓存（No-write allocate（不按写分配））。

**Write Through 策略中写数据库是同步的** ，这对于性能来说会有比较大的影响。

###  Write Back（写回）策略

这个策略的核心思想是 **在写入数据时只写入缓存，并且把缓存块儿标记为 「脏」 的**。而脏块儿只有被再次使用时才会将其中的数据写入到后端存储中。

**需要注意的是，** 在 `Write Miss` 的情况下，我们采用的是 `Write Allocate` 的方式，也就是在写入后端存储的同时要写入缓存，这样我们在之后的写请求中都只需要更新缓存即可，而无需更新后端存储了。

无论是操作系统层面的 Page Cache，还是日志的异步刷盘，亦或是消息队列中消息的异步写入磁盘，大多采用了这种策略。

### 总结

一般业务系统用的都是 Cache Aside 策略，其他两种了解即可。

## 缓存高可用

redis 分片技术可以参考 ：[Redis进阶 - 高可拓展：分片技术（Redis Cluster）详解](https://pdai.tech/md/db/nosql-redis/db-redis-x-cluster.html)

关于一致性 hash 算法： 

![image-20220309161908991](/Users/tianyou/Library/Application Support/typora-user-images/image-20220309161908991.png)

在这个算法中，我们将整个 Hash 值空间组织成一个虚拟的圆环，然后将缓存节点的 IP 地址或者主机名做 Hash 取值后，放置在这个圆环上。当我们需要确定某一个 Key 需要存取到哪个节点上的时候，先对这个 Key 做同样的 Hash 取值，确定在环上的位置，然后按照顺时针方向在环上 `行走`，遇到的第一个缓存节点就是要访问的节点。

如果删除了某个节点，原本在这个节点的 key 会顺序跑到下一个节点去进行存取。而大部分的 Key 命中的节点还是会保持不变。

同时可以引入虚拟节点，它可以将一个缓存节点计算多个 Hash 值分散到圆环的不同位置，这样既实现了数据的平均，而且当某一个节点故障或者退出的时候，它原先承担的 Key 将以更加平均的方式分配到其他节点上，从而避免雪崩的发生。

一致性 hash 算法也可能会出现脏数据问题，比如有节点 A B，A 存了一个 key 值为 3，此时 A 宕机了，客户端要更新 key，这时就会去 B 上将 key 更新为 4，然后 A 连回来了，下次客户端查询 key 就会从 A 上拿到 3。这就是脏数据。**所以，在使用一致性 Hash 算法时一定要设置缓存的过期时间，** 这样当发生漂移时，之前存储的脏数据可能已经过期，就可以减少存在脏数据的几率。

## 缓存穿透

布隆过滤器。

## CDN 静态资源加速

静态资源访问的关键点是 **就近访问**。

### CDN 的关键技术

CDN（Content Delivery Network/Content Distribution Network，内容分发网络）。简单来说，CDN 就是将静态的资源分发到，位于多个地理位置机房中的服务器上，因此它能很好地解决数据就近访问的问题，也就加快了静态资源的访问速度。

1. DNS 技术是 CDN 实现中使用的核心技术，可以将用户的请求映射到 CDN 节点上；
2. DNS 解析结果需要做本地缓存，降低 DNS 解析过程的响应时间；
3. GSLB（Global Server Load Balance，全局负载均衡） 可以给用户返回一个离着他更近的节点，加快静态资源的访问速度。



CDN 回源是由 CDN 触发的，配置 CDN 的时候需要配置源站地址。



## 数据迁移

### 双写方案

1. 将新的库配置为源库的从库，用来同步数据；

   如果需要将数据同步到多库多表，那么可以使用一些第三方工具获取 Binlog 的增量日志（比如开源工具 Canal），在获取增量日志之后就可以按照分库分表的逻辑写入到新的库表中了。

2. 同时，我们需要改造业务代码，在数据写入的时候，不仅要写入旧库，也要写入新库。

   当然，基于性能的考虑，我们可以异步地写入新库，只要保证旧库写入成功即可。 **但是，我们需要注意的是，** 需要将写入新库失败的数据记录在单独的日志中，这样方便后续对这些数据补写，保证新库和旧库的数据一致性。

3. 然后，我们就可以开始校验数据了。由于数据库中数据量很大，做全量的数据校验不太现实。你可以抽取部分数据，具体数据量依据总体数据量而定，只要保证这些数据是一致的就可以。

4. 如果一切顺利，我们就可以将读流量切换到新库了。

   由于担心一次切换全量读流量可能会对系统产生未知的影响，所以这里 **最好采用灰度的方式来切换，** 比如开始切换 10% 的流量，如果没有问题再切换到 50% 的流量，最后再切换到 100%。

5. 由于有双写的存在，所以在切换的过程中出现任何的问题，都可以将读写流量随时切换到旧库去，保障系统的性能。

6. 在观察了几天发现数据的迁移没有问题之后，就可以将数据库的双写改造成只写新库，数据的迁移也就完成了。

![img](https://zq99299.github.io/note-architect/assets/img/ad9a4aa37afc39ebe0c91144d5ef7630.ad9a4aa3.jpg)



**其中，最容易出问题的步骤就是数据校验的工作，** 所以，我建议你在未开始迁移数据之前先写好数据校验的工具或者脚本，在测试环境上测试充分之后，再开始正式的数据迁移。

如果是将数据从自建机房迁移到云上，你也可以使用这个方案， **只是你需要考虑的一个重要的因素是：** 自建机房到云上的专线的带宽和延迟，你需要尽量减少跨专线的读操作，所以在切换读流量的时候，你需要保证自建机房的应用服务器读取本机房的数据库，云上的应用服务器读取云上的数据库。这样在完成迁移之前，只要将自建机房的应用服务器停掉，并且将写入流量都切到新库就可以了。

![img](https://zq99299.github.io/note-architect/assets/img/b88aefdb07049f2019c922cdb9cb3154.b88aefdb.jpg)

这种方式的 **好处是：** 迁移的过程可以随时回滚，将迁移的风险降到了最低。 **劣势是：** 时间周期比较长，应用有改造的成本。



### 级联同步方案

步骤如下：

1. 先将新库配置为旧库的从库，用作数据同步；
2. 再将一个备库配置为新库的从库，用作数据的备份；
3. 等到三个库的写入一致后，将数据库的读流量切换到新库；
4. 然后暂停应用的写入，将业务的写入流量切换到新库（由于这里需要暂停应用的写入，所以需要安排在业务的低峰期）。

![img](https://zq99299.github.io/note-architect/assets/img/3a2e08181177529c3229c789c2081b2b.3a2e0818.jpg)

**这种方案的回滚方案也比较简单，** 可以先将读流量切换到备库，再暂停应用的写入，将写流量切换到备库，这样所有的流量都切换到了备库，也就是又回到了自建机房的环境，就可以认为已经回滚了。

![img](https://zq99299.github.io/note-architect/assets/img/ada8866fda3c3264f495c97c6214ebb9.ada8866f.jpg)

上面的级联迁移方案可以应用在，将 MySQL 从自建机房迁移到云上的场景，也可以应用在将 Redis 从自建机房迁移到云上的场景， **如果你有类似的需求可以直接拿来应用。**

这种方案 **优势是** 简单易实施，在业务上基本没有改造的成本；

**缺点是 ** 在切写的时候需要短暂的停止写入，对于业务来说是有损的，不过如果在业务低峰期来执行切写，可以将对业务的影响降至最低。

### 数据迁移时如何预热缓存

#### 改造副本组方案预热缓存

改造后的方案对读写缓存的方式进行改造，步骤是这样的：

1. 在云上部署多组 mc 的副本组，自建机房在接收到写入请求时，会优先写入自建机房的缓存节点，异步写入云上部署的 mc 节点；
2. 在处理自建机房的读请求时，会指定一定的流量，比如 10%，优先走云上的缓存节点，这样虽然也会走专线穿透回自建机房的缓存节点，但是流量是可控的；
3. 当云上缓存节点的命中率达到 90% 以上时，就可以在云上部署应用服务器，让云上的应用服务器完全走云上的缓存节点就可以了。

![img](https://zq99299.github.io/note-architect/assets/img/7f41a529a322e396232ac7963ec082f4.7f41a529.jpg)

使用了这种方式，我们可以实现缓存数据的迁移，又可以尽量控制专线的带宽和请求的延迟情况， **你也可以直接在项目中使用。**

### 小结

双写的方案是数据库、Redis 迁移的通用方案， **你可以在实际工作中直接加以使用。** 双写方案中最重要的，是通过数据校验来保证数据的一致性，这样就可以在迁移过程中随时回滚；

如果你需要将自建机房的数据迁移到云上，那么也可以考虑 **使用级联复制的方案，** 这种方案会造成数据的短暂停写，需要在业务低峰期执行；

缓存的迁移重点，是保证云上缓存的命中率，你可以 **使用改进版的副本组方式来迁移，** 在缓存写入的时候，异步写入云上的副本组，在读取时放少量流量到云上副本组，从而又可以迁移部分数据到云上副本组，又能尽量减少穿透给自建机房造成专线延迟的问题。

**如果你作为项目的负责人，** 那么在迁移的过程中，你一定要制定周密的计划：如果是数据库的迁移，那么数据的校验应该是你最需要花费时间来解决的问题。

如果是自建机房迁移到云上，那么专线的带宽一定是你迁移过程中的一个瓶颈点，你需要在迁移之前梳理清楚，有哪些调用需要经过专线，占用带宽的情况是怎样的，带宽的延时是否能够满足要求。你的方案中也需要尽量做到在迁移过程中，同机房的服务，调用同机房的缓存和数据库，尽量减少对于专线带宽资源的占用。