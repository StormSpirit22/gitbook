# 实战

## 计数系统设计

计数数据：

1. 微博的评论数、点赞数、转发数、浏览数、表态数等等；
2. 用户的粉丝数、关注数、发布微博数、私信数等等。

### 计数在业务上的特点

1. 数据量巨大。
2. 访问量大，对于性能的要求高。
3. 对于可用性、数字的准确性要求高。

### 设计

以微博 ID 为主键，转发数、评论数、点赞数和浏览数分别为单独一列，这样在获取计数时用一个 SQL 语句就搞定了。

```sql
select repost_count, comment_count, praise_count, view_count from t_weibo_count where weibo_id = ?
```

分库分表，用 「weibo_id」作为分区键。因为越是最近发布的微博，计数数据的访问量就越大，所以按照时间来分库分表会造成数据访问的不均匀。因此选用哈希的方式来做分库分表更合适。

![img](https://zq99299.github.io/note-architect/assets/img/508201de80dd909d8b7dff1d34be9f9c.508201de.jpg)

在数据量剧增之后，考虑完全使用 redis 来作为计数的存储组件。

![img](https://zq99299.github.io/note-architect/assets/img/7c8ed7992ec206671a18b8d537eaef62.7c8ed799.jpg)

为了降低对 redis 的写压力，可以使用消息队列，在转发微博的时候向消息队列写入一条消息，然后在消息处理程序中给这条微博的转发计数加 1。 **这里需要注意的一点，** 我们可以通过批量处理消息的方式进一步减小 Redis 的写压力，比如连续更改三次转发数可以合并成一次对 redis 写入，即一次加 3 。

### 降低计数系统的存储成本

1. 改造 redis 数据结构，降低存储空间。
2. 将 value 合并，1 个微博 id 对应一个大的计数结构。
3. 给计数服务增加 SSD 磁盘，然后将时间上比较久远的数据 dump 到磁盘上，内存中只保留最近的数据。当我们要读取冷数据的时候，使用单独的 I/O 线程异步地将冷数据从 SSD 磁盘中加载到一块儿单独的 Cold Cache（冷缓存） 中。
   ![img](https://zq99299.github.io/note-architect/assets/img/16cb144c96a0ab34214c966f686c9693.16cb144c.jpg)

## 未读数系统设计

### 系统通知的未读数设计

系统通知存储在一个大的列表中，这个列表对所有用户共享，也就是所有人看到的都是同一份系统通知的数据。不过不同的人最近看到的消息不同，所以每个人会有不同的未读数。因此，可以记录一下在这个列表中每个人看过最后一条消息的 ID，然后统计这个 ID 之后有多少条消息，这就是未读数了。

![img](https://zq99299.github.io/note-architect/assets/img/a5f0b6776246dc6b4c7e96c72d74a210.a5f0b677.jpg)

这个方案在实现时有这样几个关键点：

- 用户访问系统通知页面需要设置未读数为 0，我们需要将用户最近看过的通知 ID 设置为最新的一条系统通知 ID；
- 如果最近看过的通知 ID 为空，则认为是一个新的用户，返回未读数为 0；
- 对于非活跃用户，比如最近一个月都没有登录和使用过系统的用户，可以把用户最近看过的通知 ID 清空，节省内存空间。

或者红点通知场景下：

首先，我们为每一个用户存储一个时间戳，代表该用户最近点过这个红点的时间，用户点了红点，就把这个时间戳设置为当前时间；然后，我们也记录一个**全局的时间戳**，这个时间戳标识最新的一次打点时间，如果你在后台操作给全体用户打点，就更新这个时间戳为当前时间。而我们在判断是否需要展示红点时，只需要判断用户的时间戳和全局时间戳的大小，如果用户时间戳小于全局时间戳，代表在用户最后一次点击红点之后又有新的红点推送，那么就要展示红点，反之，就不展示红点了。

这两个场景的共性是 **全部用户共享一份有限的存储数据**，每个人只记录自己在这份存储中的偏移量，就可以得到未读数了。

### 信息流的未读数设计

- 首先，在通用计数器中记录每一个用户发布的博文数；

- 然后在 Redis 或者 Memcached 中记录一个人所有关注人的**博文数快照**，他关注所有人的博文总数减去快照中的博文总数就是他的**信息流未读数**。

- 当用户点击未读消息重置未读数为 0 时，将他关注所有人的博文数刷新到快照中；

  

![img](https://zq99299.github.io/note-architect/assets/img/a563b121ae1147a2d877a7bb14c9658a.a563b121.jpg)

缺陷：比如说快照中需要存储关注关系，如果关注关系变更的时候更新不及时，那么就会造成未读数不准确；快照采用的是全缓存存储，如果缓存满了就会剔除一些数据，那么被剔除用户的未读数就变为 0 了。但是好在用户对于未读数的准确度要求不高（未读 10 条还是 11 条，其实用户有时候看不出来），因此，这些缺陷也是可以接受的。

### 小结

- 评论未读、@未读、赞未读等一对一关系的未读数可以使用上一章的通用计数方案来解决；

  通用计数器：一个用户一类的计数用一个字段存储就可以了，相对简单，就是存储量很大。

- 在系统通知未读、全量用户打点等存在有限的共享存储的场景下，可以通过记录用户上次操作的时间或者偏移量，来实现未读方案；

  内容共享，通过最后一次读时间或则 id 的偏移量来计算，要注意的是：这种未读不是指你有 10 篇没有读的文章，读取一篇就减少 1 。这种未读统计，而是你总共有多少未读消息数量，点击后就重置为 0 了。

- 最后，信息流未读方案最为复杂，采用的是记录用户博文数快照的方式。

  这里和系统全量打点的方式类似，也是点击未读后，计数器就重置为 0 了。但是它有关系计算，这是和全量打点方式不一样的地方。

  

## 通用信息流系统的推模式设计

推模式是指用户发送一条微博后，主动将这条微博推送给他的粉丝，从而实现微博的分发，也能以此实现微博信息流的聚合。

假设微博系统是一个邮箱系统，那么用户发送的微博可以认为是进入到一个发件箱，用户的信息流可以认为是这个人的收件箱。推模式的做法是在用户发布一条微博时，除了往自己的发件箱里写入一条微博，同时也会给他的粉丝收件箱里写入一条微博。

### 问题和解决思路

- 消息延迟。

对明星来说，如果在发微博的同时还要将微博写入到上千万人的收件箱中，那么发微博的响应时间会非常慢，用户根本没办法接受。因此，我们一般会使用**消息队列**来消除写入的峰值，但即使这样，由于写入收件箱的消息实在太多，你还是有可能在几个小时之后才能够看到明星发布的内容，这会非常影响用户的使用体验。

为了尽量减少微博写入的延迟，可以：

1. 启动多个线程并行地处理微博写入的消息。
2. 由于消息流在展示时可以使用缓存来提升读取性能，应该尽量保证数据写入数据库的性能，必要时可以采用写入性能更好的数据库存储引擎（如 TokuDB、LSM树相关的 db ）。

- 存储成本高。

除了选择压缩率更高的存储引擎之外，还可以定期地清理数据，因为微博的数据有比较明显的实效性，用户更加关注最近几天发布的数据，通常不会翻阅很久之前的微博，所以你可以定期地清理用户的收件箱，比如只保留最近 1 个月的数据就可以了。

或者取消关注某人不需要去清理自己的收件箱删除微博，在读取自己信息流的时候，判断每一条微博是否被删除以及你是否还关注这条微博的作者，如果没有的话，就不展示这条微博的内容了。

### 小结

1. 推模式就是在用户发送微博时，主动将微博写入到他的粉丝的收件箱中；
2. 推送信息是否延迟、存储的成本、方案的可扩展性以及针对取消关注和微博删除的特殊处理是推模式的主要问题；
3. 推模式比较适合粉丝数有限的场景。



## 通用信息流系统的拉模式设计

拉模式，就是指用户主动拉取他关注的所有人的微博，将这些微博按照发布时间的倒序进行排序和聚合之后，生成信息流数据的方法。

这种情况下，每个用户有一个发件箱，当他发微博时，只需要写入自己的发件箱，当粉丝在获取信息流的时候，就可以直接去自己的关注列表里查询对应用户的发件箱了。

比如：

假设用户 A 关注了用户 B、C、D，那么当用户 B 发送一条微博的时候，他会执行这样的操作：

```sql
insert into outbox(userId, feedId, create_time) values("B", $feedId, $current_time); // 写入 B 的发件箱
```

当用户 A 想要获取他的信息流的时候，就要聚合 B、C、D 三个用户收件箱的内容了：

```sql
select feedId from outbox where userId in (select userId from follower where fanId = "A") order by create_time desc
```

优势：

- 解决了推送延迟的问题。不需要把微博推送到收件箱。
- 存储成本降低。
- 扩展性更好。比如用户将关注的人分组，那么只需要查询这个分组下所有用户，然后查询这些用户的发件箱，再把发件箱中的数据，按照时间倒序重新排序聚合就好了。

缺点：

需要对多个发件箱的数据做聚合，这个查询和聚合的成本比较高。微博的关注上限是 2000，假如你关注了 2000 人，就要查询这 2000 人发布的微博信息，然后再对查询出来的信息做聚合。

解决方案：可以缓存每个用户最近 5 天发布的微博 ID （97% 的用户都是在浏览最近 5 天之内的微博），在每次聚合时并行从这几个缓存节点中批量查询多个用户的微博 ID，获取到之后再在应用服务内存中排序后就好了。

缓存节点的带宽成本比较高。

解决方案：使用副本降低主缓存的带宽。在部署缓存副本之后，请求会先查询副本中的数据，只有不命中的请求才会查询主缓存的数据。假如原本缓存带宽是 100M，我们部署 4 组缓存副本，缓存副本的命中率是 60%，那么主缓存带宽就降到 100M * 40% = 40M，而每组缓存副本的带宽为 100M / 4 = 25M，这样每一组缓存的带宽都降为可接受的范围之内了。

![img](https://zq99299.github.io/note-architect/assets/img/679c081c73c30ccc6dafc3f2cae0a13a.679c081c.jpg)

### 推拉结合的方案
